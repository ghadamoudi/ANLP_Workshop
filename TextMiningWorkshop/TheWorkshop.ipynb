{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Text Mining Workshop\n",
    "\n",
    "In this workshop, an introduction to Arabic text mining in Pythin is introduced.\n",
    "\n",
    "## I. Arabic Natural Language Processing\n",
    "\n",
    "Arabic language is ranked as the fifth most spoken language in the world. Around 414 million people speak Arabic as the first language, with 219 million are Internet users.\n",
    "Arabic Natural Language Processing (ANLP) is gaining more popularity due to that large amount of user generated text online. Thus, the need for tools for analyzing and understanding Arabic text is now greater than ever before. Different areas for ANLP include:\n",
    "\n",
    "- machine translation, \n",
    "- sentiment analysis, \n",
    "- and question answering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulties with ANLP  \n",
    "1.\tOrthographic ambiguity عَقد ام عِقد.. الغموض الإملائي\n",
    "2.\tMorphological richness  ذهب ذهبت ذهبوا .. الغنى الصرفي\n",
    "3.\tDialectal variation تعدد اللهجات\n",
    "4.\tSpelling mistakes الأخطاء الإملائية\n",
    "5.\tScarcity in tools and scientific research قلة الأدوات البرمجية والبحث العلمي"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Python\n",
    "\n",
    "- Popularity. (not necessarily the most important)\n",
    "- The fastest-growing major programming language. (Check it out in Google Trends)\n",
    "- Programming language of the year for 2018 (TIOBE), and top ranking by IEEE Spectrum and PyPL.\n",
    "- The most popular language for introductory computer science courses at the top American colleges.\n",
    "- The official teaching language for high schools in France.\n",
    "\n",
    "### Data, Types and Variables¶\n",
    "\n",
    "1. **Boolean**: bool, immutable, examples: True, False\n",
    "2. **Integer**: int, immutable, 47, 25000, 25_000\n",
    "3. **Floating point**: float, immutable, 3.14, 2.7e5\n",
    "4. **Complex**:\tcomplex, immutable, 3j, 5 + 9j\n",
    "5. **Text string**:\tstr, immutable,\t'alas', \"alack\", '''a verse attack'''\n",
    "6. **List**: list, mutable,\t['Winken', 'Blinken', 'Nod']\n",
    "7. **Tuple**: tuple, immutable,\t(2, 4, 8)\n",
    "8. **Set**:\tset, mutable, {3, 5, 7}\n",
    "9. **Dictionary**: dict, mutable, {'game': 'bingo', 'dog': 'dingo', 'drummer': 'Ringo'}\n",
    "\n",
    "### If else, Comments, and other Stuff.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woe!\n"
     ]
    }
   ],
   "source": [
    "# This is a comment.. \n",
    "# if else, is the same as other programming languages, just remeber the colon (:)\n",
    "\n",
    "disaster = True\n",
    "\n",
    "if disaster:\n",
    "    # Indentation in Python is mandatory to define the blocks of statements. \n",
    "    print(\"Woe!\")\n",
    "else:\n",
    "    print(\"Whee!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There was a Young Lady of Norway,\\nWho casually sat in a doorway;\\nWhen the door squeezed her flat,\\nShe exclaimed, \"What of that?\"\\nThis courageous Young Lady of Norway.'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem =  '''There was a Young Lady of Norway,\n",
    "Who casually sat in a doorway;\n",
    "When the door squeezed her flat,\n",
    "She exclaimed, \"What of that?\"\n",
    "This courageous Young Lady of Norway.'''\n",
    "poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a Young Lady of Norway,\n",
      "Who casually sat in a doorway;\n",
      "When the door squeezed her flat,\n",
      "She exclaimed, \"What of that?\"\n",
      "This courageous Young Lady of Norway.\n"
     ]
    }
   ],
   "source": [
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'98.6'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting to string\n",
    "\n",
    "str(98.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of a string\n",
    "\n",
    "len(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is one of my favorite shops..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is one my favorite shops..'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.replace(' of ', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists, Dictionaries and Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Groucho', 'Chico', 'Wanda']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marxes = ['Groucho', 'Chico', 'Harpo']\n",
    "marxes[2] = 'Wanda'\n",
    "marxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary\n",
    "\n",
    "A dictionalry is a comma-separated key : value pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'apple', 2: 'ball'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dictionary with integer keys\n",
    "\n",
    "my_dict = {1: 'apple', 2: 'ball'}\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Salem': 'Rana',\n",
       " 'Saber': 'Mona',\n",
       " 'Majid': 'Sama',\n",
       " 'Raed': 'Dana',\n",
       " 'Samer': 'Sara'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that dictionary keys have to be unique\n",
    "\n",
    "students = {\n",
    "'Salem': 'Rana',\n",
    "'Saber': 'Mona',\n",
    "'Majid': 'Sama',\n",
    "'Raed' : 'Dana',\n",
    "'Samer': 'Sara',\n",
    "}\n",
    "students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sets\n",
    "\n",
    "- Like a dictionary, with only the keys, where each key must be unique. \n",
    "- You use a set when you only want to know that something exists.\n",
    "- Set theory was taught in elementary school with basic mathematics and set operations such as union and intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 2, 4, 6, 8}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_numbers = {0, 2, 4, 6, 8}\n",
    "even_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quack\n"
     ]
    }
   ],
   "source": [
    "# defining a function with: def function_name() and a colon:\n",
    "\n",
    "def make_a_sound():\n",
    "    \n",
    "    print('quack')\n",
    "    \n",
    "# calling the function\n",
    "\n",
    "make_a_sound()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy and Pandas\n",
    "**NumPy**: Numerical Python, is one of the most important foundational packages for numerical computing in Python. Most computational packages providing scientific functionality use NumPy’s array objects. \n",
    "\n",
    "**Pandas**: pandas is a widely used python library, it is powerful, flexible, open source data analysis and manipulation tool, built on top of the Python programming language.\n",
    "\n",
    "**Panda Dataframe**\n",
    "- Represents a rectangular table of data and contains an ordered collection of columns.\n",
    "- Each column can be a different value type (numeric, string, boolean, etc.). \n",
    "- The DataFrame has both a row and column index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>year</th>\n",
       "      <th>pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>2000</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>2001</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>2002</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>2001</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>2002</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>2003</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state  year  pop\n",
       "0    Ohio  2000  1.5\n",
       "1    Ohio  2001  1.7\n",
       "2    Ohio  2002  3.6\n",
       "3  Nevada  2001  2.4\n",
       "4  Nevada  2002  2.9\n",
       "5  Nevada  2003  3.2"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],\n",
    "        'year': [2000, 2001, 2002, 2001, 2002, 2003],\n",
    "        'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\n",
    "frame = pd.DataFrame(data)\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary building block for a DataFrame is the Series, making up its rows and columns. The Pandas Series data structure is a one-dimensional labelled array. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Getting Data\n",
    "There are multiple sources for Twitter data:\n",
    "1. Collect your own, using:\n",
    "    - Twitter API, streaming or search APIs.\n",
    "    - Netlytic: a cloud-based text and social networks analyzer. Automatically summarize textual data and discover communication networks from publicly accessible social media posts [1].\n",
    "2. Find a dataset online. There are plenty of sources for free datasets. Some of my favaorite are Paperswithcode [2], SNAP [3], and Kaggle [4].\n",
    "\n",
    "Depending on the source of data, you will need some data reshaping and parsing. For example, if you get your data from Twitter API, it will be in JSON format, so you will need some steps to convert the data from JSON to a Pandas dataframe. But don't worry, if you are working with Python, you will find many tutorials online that show you how to parse and clean the data.\n",
    "\n",
    "#### ASTD: Arabic Sentiment Tweets Dataset\n",
    "\n",
    "This dataset contains over 10k Arabic sentiment tweets classified into four classes subjective positive, subjective negative,\n",
    "subjective mixed, and objective. Two sets of baseline sentiment analysis experiments are supported with the dataset.\n",
    "\n",
    "https://paperswithcode.com/paper/astd-arabic-sentiment-tweets-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بعد استقالة رئيس #المحكمة_الدستورية ننتظر استقالة #رئيس_القضاء #السودان</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>أهنئ الدكتور أحمد جمال الدين، القيادي بحزب مصر، بمناسبة صدور أولى روايته</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>البرادعي يستقوى بامريكا مرةاخرى و يرسل عصام العريان الي واشنطن شئ مقرف</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#الحرية_والعدالة | شاهد الآن: #ليلة_الاتحادية أول فيلم استقصائي يتناول أسرار و كواليس تعرض لأول مرة حول حقيقة</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الوالدة لو اقولها بخاطري حشيشة تضحك بس من اقولها ملل الله وكيلك تعطيني محاضرة عن الفسق والفجور بجنوب الشيشان #ليه كذا يانبع الحنان</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                tweet  \\\n",
       "0                                                             بعد استقالة رئيس #المحكمة_الدستورية ننتظر استقالة #رئيس_القضاء #السودان   \n",
       "1                                                            أهنئ الدكتور أحمد جمال الدين، القيادي بحزب مصر، بمناسبة صدور أولى روايته   \n",
       "2                                                              البرادعي يستقوى بامريكا مرةاخرى و يرسل عصام العريان الي واشنطن شئ مقرف   \n",
       "3                       #الحرية_والعدالة | شاهد الآن: #ليلة_الاتحادية أول فيلم استقصائي يتناول أسرار و كواليس تعرض لأول مرة حول حقيقة   \n",
       "4  الوالدة لو اقولها بخاطري حشيشة تضحك بس من اقولها ملل الله وكيلك تعطيني محاضرة عن الفسق والفجور بجنوب الشيشان #ليه كذا يانبع الحنان   \n",
       "\n",
       "  polarity  \n",
       "0      OBJ  \n",
       "1      POS  \n",
       "2      NEG  \n",
       "3      OBJ  \n",
       "4  NEUTRAL  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/Tweets.txt', delimiter=\"\\t\", names=[\"tweet\", \"polarity\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بعد استقالة رئيس #المحكمة_الدستورية ننتظر استقالة #رئيس_القضاء #السودان</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>أهنئ الدكتور أحمد جمال الدين، القيادي بحزب مصر، بمناسبة صدور أولى روايته</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>البرادعي يستقوى بامريكا مرةاخرى و يرسل عصام العريان الي واشنطن شئ مقرف</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#الحرية_والعدالة | شاهد الآن: #ليلة_الاتحادية أول فيلم استقصائي يتناول أسرار و كواليس تعرض لأول مرة حول حقيقة</td>\n",
       "      <td>OBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>الوالدة لو اقولها بخاطري حشيشة تضحك بس من اقولها ملل الله وكيلك تعطيني محاضرة عن الفسق والفجور بجنوب الشيشان #ليه كذا يانبع الحنان</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                tweet  \\\n",
       "0                                                             بعد استقالة رئيس #المحكمة_الدستورية ننتظر استقالة #رئيس_القضاء #السودان   \n",
       "1                                                            أهنئ الدكتور أحمد جمال الدين، القيادي بحزب مصر، بمناسبة صدور أولى روايته   \n",
       "2                                                              البرادعي يستقوى بامريكا مرةاخرى و يرسل عصام العريان الي واشنطن شئ مقرف   \n",
       "3                       #الحرية_والعدالة | شاهد الآن: #ليلة_الاتحادية أول فيلم استقصائي يتناول أسرار و كواليس تعرض لأول مرة حول حقيقة   \n",
       "4  الوالدة لو اقولها بخاطري حشيشة تضحك بس من اقولها ملل الله وكيلك تعطيني محاضرة عن الفسق والفجور بجنوب الشيشان #ليه كذا يانبع الحنان   \n",
       "\n",
       "  polarity  \n",
       "0      OBJ  \n",
       "1      POS  \n",
       "2      NEG  \n",
       "3      OBJ  \n",
       "4  NEUTRAL  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing column width to see the full tweet\n",
    "\n",
    "pd.options.display.max_colwidth = 140\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Explore Your Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OBJ        6470\n",
       "NEG        1642\n",
       "NEUTRAL     805\n",
       "POS         777\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.polarity.value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9694 entries, 0 to 9693\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   tweet     9694 non-null   object\n",
      " 1   polarity  9694 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 151.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('في', 2605),\n",
       " ('من', 2500),\n",
       " ('على', 1669),\n",
       " ('و', 1450),\n",
       " ('فى', 744),\n",
       " ('#', 696),\n",
       " ('عن', 679),\n",
       " ('-', 653),\n",
       " ('لا', 641),\n",
       " ('مع', 608),\n",
       " ('مصر', 563),\n",
       " ('…', 519),\n",
       " ('أن', 513),\n",
       " ('الله', 482),\n",
       " ('ما', 482),\n",
       " ('ان', 449),\n",
       " ('كل', 440),\n",
       " ('يا', 432),\n",
       " (':', 395),\n",
       " ('#مصر', 381)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most frequent words\n",
    "# A counter tool is provided to support convenient \n",
    "# and rapid counting and summing operations.\n",
    "\n",
    "from collections import Counter\n",
    "Counter(\" \".join(data[\"tweet\"]).split()).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people', 'hashtags', 'topics']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = 'In social media, #people use #hashtags to highlight #topics in a tweet'\n",
    "re.findall(r'#(\\w+)', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example we use the method \"findall\" of the class \"re\" to find a certain pattern in a string which in our case a word starts with #. Next we will use the same patters, however we will use a different method \"extractall\" which is a pandas series method. Since a column in a dataframe is considered a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = data.tweet.str.extractall(r'(\\#\\w+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>#المحكمة_الدستورية</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#رئيس_القضاء</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#السودان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>0</th>\n",
       "      <td>#الحرية_والعدالة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#ليلة_الاتحادية</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "  match                    \n",
       "0 0      #المحكمة_الدستورية\n",
       "  1            #رئيس_القضاء\n",
       "  2                #السودان\n",
       "3 0        #الحرية_والعدالة\n",
       "  1         #ليلة_الاتحادية"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#مصر                 390\n",
       "#السادة_المحترمون    224\n",
       "#السعودية            141\n",
       "#يوسف_الحسيني        105\n",
       "#الرياض               89\n",
       "#الهلال               87\n",
       "#قطر                  85\n",
       "#السيسي               78\n",
       "#النصر                78\n",
       "#الكويت               71\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags = hashtags[0].value_counts()\n",
    "hashtags.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet       0\n",
       "polarity    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check NaNs\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Duplicates\n",
    "\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Data Preprocessing\n",
    "1. Removing duplicates\n",
    "2. Removing punctuation, hashtags and mentions\n",
    "3. Removing diacritics & normalizing vowels\n",
    "4. Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove Duplicates\n",
    "\n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. All Punctuations\n",
    "\n",
    "import string\n",
    "\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ»«•'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "# helpful function\n",
    "\n",
    "def strip_tags_and_punctuations(text):\n",
    "    mention_and_hashtag_prefixes = ['@','#']\n",
    "\n",
    "    # remove mentions and hashtags\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if len(word) != 1:\n",
    "            if word[0] not in mention_and_hashtag_prefixes and word[1] not in mention_and_hashtag_prefixes: \n",
    "                words.append(word)\n",
    "        else:\n",
    "            if word[0] not in mention_and_hashtag_prefixes:\n",
    "                words.append(word)\n",
    "        text = ' '.join(words)\n",
    "    \n",
    "    # replace all punctuations except the above with space\n",
    "    for separator in  punctuations_list:\n",
    "        if separator not in mention_and_hashtag_prefixes:\n",
    "            text = text.replace(separator,' ')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAMeL Tools\n",
    "\n",
    "For some of our preprocessing, we will use CAMeL Tools, a specialized natural language processing tool for Arabic preprocessing. CAMeL Tools is a suite of Arabic NLP tools developed by the CAMeL Lab at New York University Abu Dhabi. [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to install the library once.\n",
    "#!pip install camel-tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Removing diacritics & normalizing vowels\n",
    "\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_ar\n",
    "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
    "\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "\n",
    "import demoji\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dediac(text):\n",
    "    \n",
    "    text = normalize_alef_maksura_ar(text)\n",
    "    text = normalize_alef_ar(text)\n",
    "    text = normalize_teh_marbuta_ar(text)\n",
    "    \n",
    "    # Dediacritization\n",
    "    text = dediac_ar(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Removing stop words\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = nltk.corpus.stopwords.words(\"arabic\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \n",
    "    text_tokenized = word_tokenize(text)\n",
    "    text_no_stop = [word for word in text_tokenized if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(text_no_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test these functions.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: _ علاج السكري #اف_نتشورال #السعودية #العراق #اليمن #مصر #قطر #الكويت #عمان #سوريا #ليبيا 46\n",
      "After:   علاج السكري 46\n"
     ]
    }
   ],
   "source": [
    "# Testing the strip_tags_and_punctuations function..\n",
    "\n",
    "print('Before:', data['tweet'][10])\n",
    "print('After:', strip_tags_and_punctuations(data['tweet'][10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: الصداقة تزرع الحياة أزهارًا #مي_زيادة\n",
      "After: الصداقه تزرع الحياه ازهارا #مي_زياده\n"
     ]
    }
   ],
   "source": [
    "# Testing the normalize_dediac function\n",
    "\n",
    "print('Before:', data['tweet'][13])\n",
    "print('After:', normalize_dediac(data['tweet'][13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: الإصرار علي الإستفتاء في ظل وطن ممزق وغليان شعبي وغياب أمني وقضاء معطل واعلام محاصر هو انعدام للإحساس بالمسؤولية نحو مصر\n",
      "After: الإصرار علي الإستفتاء ظل وطن ممزق وغليان شعبي وغياب أمني وقضاء معطل واعلام محاصر انعدام للإحساس بالمسؤولية مصر\n"
     ]
    }
   ],
   "source": [
    "# Testing remove_stopwords function\n",
    "\n",
    "print('Before:', data['tweet'][14])\n",
    "print('After:', remove_stopwords(data['tweet'][14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the preprocessing functions to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].apply(strip_tags_and_punctuations)\n",
    "data['tweet'] = data['tweet'].apply(normalize_dediac)\n",
    "data['tweet'] = data['tweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.iloc[5:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Preprocessing & cleaning steps\n",
    "1. Removing emojis\n",
    "2. Stemming\n",
    "3. Removing numbers\n",
    "4. Removing white spaces, etc..\n",
    "5. adding new features, such as account features (about the user), or about the text..\n",
    "\n",
    "Bottom line: understand your data and your objective then decide which steps you need..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Vectorization\n",
    "\n",
    "1. Bag-of-words: A words is assigned a value corresponding to it's occurence times in document (tweet)\n",
    "2. TF IDF: Term Frequency Inverse Document Frequency\n",
    "\n",
    "### TF IDF\n",
    "\n",
    "- Each word has a TF and IDF score\n",
    "- Term frequency: **how many times the term appears in a document.** \n",
    "- Inverse Document Frequency: **reflects how important a word is to a document in a collection.** \n",
    "\n",
    "**Example** <br>\n",
    "**TF:** a document containing 100 words and the word math apears 5 times, then TF for the word math is 5/100 = 0.05. <br>\n",
    "**IDF:** if the word math appears in 10 documents and we have total of 50 document then IDF = log(50/10) = 1.6\n",
    "   \n",
    "- TF.IDF for the word math in this context is: 0.05 X 1.6 = 0.08\n",
    "- The higher the TF.IDF the rarer the term and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer (Bag-of-words)\n",
    "count = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF \n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful function\n",
    "\n",
    "def calculate_accuracies(vec, clf):\n",
    "    # fit and train\n",
    "    pipe = make_pipeline(vec, clf)\n",
    "    pipe.fit(x_train, y_train)\n",
    "\n",
    "    # evalution and generalization\n",
    "    y_train_pred = pipe.predict(x_train)\n",
    "    y_test_pred = pipe.predict(x_test)\n",
    "    \n",
    "    print('Training Accuracy: ', accuracy_score(y_train, y_train_pred))\n",
    "    print('Testing  Accuracy: ', accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['tweet'], data['polarity'],\n",
    "                                                    train_size=0.8, stratify = data['polarity'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9673632610939112\n",
      "Testing  Accuracy:  0.6754385964912281\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=120)\n",
    "calculate_accuracies(count, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9913570691434469\n",
      "Testing  Accuracy:  0.6444788441692466\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent Classifier\n",
    "sgd = SGDClassifier(loss='hinge', alpha=1e-5, verbose=False)\n",
    "calculate_accuracies(count, sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9130546955624355\n",
      "Testing  Accuracy:  0.673890608875129\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "calculate_accuracies(count, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9868421052631579\n",
      "Testing  Accuracy:  0.6491228070175439\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "svc = SVC(kernel='linear')\n",
    "calculate_accuracies(count, svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.7416150670794633\n",
      "Testing  Accuracy:  0.6790505675954592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ghadaamoudi/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=80)\n",
    "calculate_accuracies(tfidf, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9896800825593395\n",
      "Testing  Accuracy:  0.6646026831785345\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent Classifier\n",
    "sgd = SGDClassifier(loss='log', alpha=1e-5, max_iter=10, verbose=False)\n",
    "calculate_accuracies(tfidf, sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.6805985552115583\n",
      "Testing  Accuracy:  0.6676986584107327\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "calculate_accuracies(tfidf, nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.7427760577915377\n",
      "Testing  Accuracy:  0.6842105263157895\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "svc = SVC(kernel='sigmoid')\n",
    "calculate_accuracies(tfidf, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More performance measures:\n",
    "- Cross validation\n",
    "- Confusion matrix\n",
    "- Calculate precision recall and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII Fine Tune Your Model \n",
    "- To fine tune the model you can use Grid Search, which allow you to tray the model with a range of hyper parameters.\n",
    "- You can also experiment with n-grams. \n",
    "- You can try deep learning..\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Netlytic:https://netlytic.org/ <br>\n",
    "[2] Paperswithcode: https://paperswithcode.com/ <br>\n",
    "[3] SNAP: https://snap.stanford.edu/data/ <br>\n",
    "[4] Kaggle: https://www.kaggle.com/ <br>\n",
    "[5] https://camel-tools.readthedocs.io/en/latest/getting_started.html <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
